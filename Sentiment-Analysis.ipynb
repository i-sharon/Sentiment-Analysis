{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6108f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U textblob\n",
    "#!pip install vaderSentiment\n",
    "#!pip install flair\n",
    "#!pip install pycorenlp\n",
    "#!pip install happytransformer\n",
    "#!pip install afinn\n",
    "#!pip install NRCLex\n",
    "#!pip install senticnet\n",
    "#!pip install pattern\n",
    "#!pip install tweepy\n",
    "#!pip install googletrans==3.1.0a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d61f7820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae111a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f36f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_metrics(target,predicted):\n",
    "    print(\"--------------Metrics-------------------\")\n",
    "    print(classification_report(target,predicted))\n",
    "    print(\"\\n--------------CONFUSION-MATRIX-------------------\")\n",
    "    print('')\n",
    "\n",
    "    conf_mat = confusion_matrix(target,predicted)\n",
    "    print('Confusion matrix:\\n', conf_mat)\n",
    "\n",
    "    group_names = ['True Negative','False Positive','False Negative','True Positive']\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in conf_mat.flatten()]\n",
    "\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in conf_mat.flatten()/np.sum(conf_mat)]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "    plt.subplots(figsize=(10,6))\n",
    "    sns.heatmap(conf_mat, annot=labels, fmt='', cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e8e8aa",
   "metadata": {},
   "source": [
    "# TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e524d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c7fefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"sentiment140.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40524727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tb_sentiment(sentence):\n",
    "    blob = TextBlob(sentence)\n",
    "    if blob.sentiment[0]<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8833a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time()\n",
    "df['compound'] = df['text'].apply(lambda text: tb_sentiment(text))\n",
    "end=time()\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbc8d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"equal\"]=np.where(df[\"target\"] == df[\"compound\"], True, False)\n",
    "t=(df[\"equal\"]==True).sum()\n",
    "f=(df[\"equal\"]==False).sum()\n",
    "tb_acc=t/(t+f)\n",
    "tb_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b18436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tb_time=end-start\n",
    "print(\"The time taken:\",tb_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446f54b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics(df['target'],df['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fb033d",
   "metadata": {},
   "source": [
    "# VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37987764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3e256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"sentiment140.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e287ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_scores(sentence):\n",
    "    sent = SentimentIntensityAnalyzer()\n",
    "    sentiment_dict = sent.polarity_scores(sentence)\n",
    "    if sentiment_dict['compound']<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649fda20",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time()\n",
    "df['compound'] = df['text'].apply(lambda text: sentiment_scores(text))\n",
    "end=time()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0771a477",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"equal\"]=np.where(df[\"target\"] == df[\"compound\"], True, False)\n",
    "t=(df[\"equal\"]==True).sum()\n",
    "f=(df[\"equal\"]==False).sum()\n",
    "vd_acc=t/(t+f)\n",
    "vd_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b919dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "vd_time=end-start\n",
    "print(\"The time taken:\",vd_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1bad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics(df['target'],df['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dc5933",
   "metadata": {},
   "source": [
    "# Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b8fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c032a927",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"sentiment140.csv\")\n",
    "df.head()\n",
    "classifier = TextClassifier.load('en-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed974f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flair_sentiment(sentence):\n",
    "    text = Sentence(sentence)\n",
    "    classifier.predict(text)\n",
    "    label = str(text.labels[0]).split()[0]\n",
    "    if label==\"NEGATIVE\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1aba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time()\n",
    "df['compound'] = df['text'].apply(lambda text: flair_sentiment(text))\n",
    "end=time()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170bd738",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"equal\"]=np.where(df[\"target\"] == df[\"compound\"], True, False)\n",
    "t=(df[\"equal\"]==True).sum()\n",
    "f=(df[\"equal\"]==False).sum()\n",
    "flair_acc=t/(t+f)\n",
    "flair_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a221689",
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_time=end-start\n",
    "print(\"The time taken:\",flair_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a421bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics(df['target'],df['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8039dc",
   "metadata": {},
   "source": [
    "# Stanford CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55078ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ca3452",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"sentiment140.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97885ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corenlp_sentiment(sentence):\n",
    "    text=re.sub('[^A-Za-z0-9.]+', ' ',sentence)\n",
    "    nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "    results = nlp.annotate(text,properties={\n",
    "        'annotators':'sentiment, ner, pos',\n",
    "        'outputFormat': 'json',\n",
    "        'timeout': 50000,\n",
    "        })\n",
    "    sentsum=0\n",
    "    count=0\n",
    "    for s in results[\"sentences\"]:\n",
    "        sentsum+=int(s[\"sentimentValue\"])\n",
    "        count+=1\n",
    "    sentavg=sentsum/count\n",
    "    if sentavg<2:\n",
    "        return 0\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95f819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time()\n",
    "df['compound'] = df['text'].apply(lambda text: corenlp_sentiment(text))\n",
    "end=time()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ae8826",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"equal\"]=np.where(df[\"target\"] == df[\"compound\"], True, False)\n",
    "t=(df[\"equal\"]==True).sum()\n",
    "f=(df[\"equal\"]==False).sum()\n",
    "core_acc=t/(t+f)\n",
    "core_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c947cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_time=end-start\n",
    "print(\"The time taken:\",core_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca2334",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics(df['target'],df['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8001392b",
   "metadata": {},
   "source": [
    "# AFINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce686d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from afinn import Afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e134ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"sentiment140.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dba5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def afn_sentiment(sentence):\n",
    "    afinn = Afinn(language='en')\n",
    "    sent=afinn.score(sentence)\n",
    "    if sent<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f697199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time()\n",
    "df['compound'] = df['text'].apply(lambda text: afn_sentiment(text))\n",
    "end=time()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8688a82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"equal\"]=np.where(df[\"target\"] == df[\"compound\"], True, False)\n",
    "t=(df[\"equal\"]==True).sum()\n",
    "f=(df[\"equal\"]==False).sum()\n",
    "af_acc=t/(t+f)\n",
    "af_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0470a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "af_time=end-start\n",
    "print(\"The time taken:\",af_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1400be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics(df['target'],df['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b049547",
   "metadata": {},
   "source": [
    "# Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edab399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71df46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"sentiment140.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa10dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_sentiment(sentence):\n",
    "    sent=sentiment(sentence)\n",
    "    if sent[0]<=0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95692e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time()\n",
    "df['compound'] = df['text'].apply(lambda text: pattern_sentiment(text))\n",
    "end=time()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d31fab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"equal\"]=np.where(df[\"target\"] == df[\"compound\"], True, False)\n",
    "t=(df[\"equal\"]==True).sum()\n",
    "f=(df[\"equal\"]==False).sum()\n",
    "pt_acc=t/(t+f)\n",
    "pt_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63803fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_time=end-start\n",
    "print(\"The time taken:\",pt_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde8ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics(df['target'],df['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2628922a",
   "metadata": {},
   "source": [
    "# Happy transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a66d08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from happytransformer import HappyTextClassification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e6638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"sentiment140.csv\")\n",
    "df.head()\n",
    "happy_tc = HappyTextClassification(\"DISTILBERT\", \"distilbert-base-uncased-finetuned-sst-2-english\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0ac7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ht_sentiment(sentence):\n",
    "    result = happy_tc.classify_text(sentence)\n",
    "    if result.label==\"NEGATIVE\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6ee57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time()\n",
    "df['compound'] = df['text'].apply(lambda text: ht_sentiment(text))\n",
    "end=time()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd34071",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"equal\"]=np.where(df[\"target\"] == df[\"compound\"], True, False)\n",
    "t=(df[\"equal\"]==True).sum()\n",
    "f=(df[\"equal\"]==False).sum()\n",
    "ht_acc=t/(t+f)\n",
    "ht_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa162a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_time=end-start\n",
    "print(\"The time taken:\",ht_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea8d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics(df['target'],df['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51cb801",
   "metadata": {},
   "source": [
    "# NRC Lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2ec419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nrclex import NRCLex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077dd672",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"sentiment140.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11a96b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nrc_sentiment(sentence):\n",
    "    nrc = NRCLex(sentence)\n",
    "    pos=nrc.affect_frequencies['positive']\n",
    "    neg=nrc.affect_frequencies['negative']\n",
    "    if pos<=neg:\n",
    "        return 0\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e935d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time()\n",
    "df['compound'] = df['text'].apply(lambda text: nrc_sentiment(text))\n",
    "end=time()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b89628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"equal\"]=np.where(df[\"target\"] == df[\"compound\"], True, False)\n",
    "t=(df[\"equal\"]==True).sum()\n",
    "f=(df[\"equal\"]==False).sum()\n",
    "nrc_acc=t/(t+f)\n",
    "nrc_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acfa39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrc_time=end-start\n",
    "print(\"The time taken:\",nrc_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5e2f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics(df['target'],df['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549ca530",
   "metadata": {},
   "source": [
    "# Senticnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cead386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from senticnet.senticnet import SenticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd87ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"sentiment140.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929840ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snet_sentiment(sentence):\n",
    "    word=sentence.split()\n",
    "    sent=0\n",
    "    sn = SenticNet()\n",
    "    for w in word:\n",
    "        try:\n",
    "            polarity_value = sn.polarity_value(w)\n",
    "        except:\n",
    "            pass\n",
    "        else:\n",
    "            sent+=float(polarity_value)\n",
    "    if sent<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e04fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time()\n",
    "df['compound'] = df['text'].apply(lambda text: snet_sentiment(text))\n",
    "end=time()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41af0647",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"equal\"]=np.where(df[\"target\"] == df[\"compound\"], True, False)\n",
    "t=(df[\"equal\"]==True).sum()\n",
    "f=(df[\"equal\"]==False).sum()\n",
    "snet_acc=t/(t+f)\n",
    "snet_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e99b03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "snet_time=end-start\n",
    "print(\"The time taken:\",snet_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068f9dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics(df['target'],df['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3951998",
   "metadata": {},
   "source": [
    "# Bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df41790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "models=('TextBlob','Vader','Flair','Stanford CoreNLP','AFINN','Pattern','Happy Transformer','NRC Lex','Senticnet')\n",
    "acc_values=(tb_acc,vd_acc,flair_acc,core_acc,af_acc,pt_acc,ht_acc,nrc_acc,snet_acc)\n",
    "fig = plt.figure(figsize = (15, 5))\n",
    "plt.bar(models, acc_values, color ='green',width = 0.4)\n",
    "plt.xlabel(\"Lexicon Based Models\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Comparison Of Models\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2d6f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "models=('TextBlob','Vader','Flair','Stanford CoreNLP','AFINN','Pattern','Happy Transformer','NRC Lex','Senticnet')\n",
    "time_values=(tb_time,vd_time,flair_time,core_time,af_time,pt_time,ht_time,nrc_time,snet_time)\n",
    "fig = plt.figure(figsize = (15, 5))\n",
    "plt.bar(models, time_values, color ='green',width = 0.4)\n",
    "plt.xlabel(\"Lexicon Based Models\")\n",
    "plt.ylabel(\"time\")\n",
    "plt.title(\"Comparison Of Models\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632eca01",
   "metadata": {},
   "source": [
    "# Tweet collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a7b4a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator, constants\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "736e58e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b06c093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key='9N25TnKAVRlCnzwczo1H57AC9'\n",
    "consumer_secret='75jWEcEvlDnEz49QkquoccNoaHHFkzAyF2OUByVqK0UpnuqllT'\n",
    "access_token='1186607165944913920-BUl3h3wuCr8y7YNmXliBZUlBnS0GjG'\n",
    "access_token_secret='uNLDlE1TVJ3bOkZbx85TbH2TFY4lP3Yjr04rW0QDs7sef'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8e148615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import re\n",
    "import pandas as pd\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "regex = \"[^a-zA-Z0-9]+\"\n",
    "p=re.compile(regex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8c4c4bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet(keyword):\n",
    "    search_words = keyword\n",
    "    date_since = \"2020-11-16\"\n",
    "    tweets = tweepy.Cursor(api.search_tweets,\n",
    "                  q=search_words,\n",
    "                  lang=\"\",\n",
    "                ).items(5)\n",
    "\n",
    "    tweet_det=[]\n",
    "    tweet_transdet=[]\n",
    "    for tweet in tweets:\n",
    "        tweet_det.append([tweet.id,tweet.lang,tweet.text,tweet.user.screen_name,tweet.user.name,tweet.created_at,tweet.user.location])\n",
    "        if tweet.lang!=\"en\":\n",
    "            tw_trans = translator.translate(tweet.text)\n",
    "            tweet_transdet.append([tweet.id,tweet.lang,tw_trans.text,tweet.user.screen_name,tweet.user.name,tweet.created_at,tweet.user.location])\n",
    "        else:\n",
    "            tweet_transdet.append([tweet.id,tweet.lang,tweet.text,tweet.user.screen_name,tweet.user.name,tweet.created_at,tweet.user.location])\n",
    "\n",
    "    tweet_details = pd.DataFrame(data=tweet_det, columns=['Id','Lang','Tweet','Username','User','Date-Time','Location'])\n",
    "    #tweet_details.to_csv(keyword+\".csv\")\n",
    "    tweet_transdetails = pd.DataFrame(data=tweet_transdet, columns=['Id','Lang','Tweet','Username','User','Date-Time','Location'])\n",
    "    return tweet_transdetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "aabb5ff3",
   "metadata": {},
   "outputs": [
    {
     "ename": "Forbidden",
     "evalue": "403 Forbidden\n195 - Missing or invalid url parameter.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-473a6beeeb43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mkey1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\" #\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mkey1_tweet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_tweet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mkey1_tweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-77-06cb4112edf3>\u001b[0m in \u001b[0;36mget_tweet\u001b[1;34m(keyword)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtweet_det\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtweet_transdet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mtweet_det\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscreen_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreated_at\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;34m\"en\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tweepy\\cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tweepy\\cursor.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m             \u001b[1;31m# Reached end of current page, get the next page...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tweepy\\cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tweepy\\cursor.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRawParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;31m# This is a special invocation that returns the underlying\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tweepy\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;33m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpagination_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tweepy\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'payload_list'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpayload_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'payload_type'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpayload_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpayload_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpayload_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpayload_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpayload_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tweepy\\api.py\u001b[0m in \u001b[0;36msearch_tweets\u001b[1;34m(self, q, **kwargs)\u001b[0m\n\u001b[0;32m   1263\u001b[0m         \u001b[0mhttps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mdeveloper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtwitter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0men\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtwitter\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mreference\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \"\"\"\n\u001b[1;32m-> 1265\u001b[1;33m         return self.request(\n\u001b[0m\u001b[0;32m   1266\u001b[0m             'GET', 'search/tweets', endpoint_parameters=(\n\u001b[0;32m   1267\u001b[0m                 \u001b[1;34m'q'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'geocode'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lang'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'locale'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'result_type'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'count'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tweepy\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, endpoint, endpoint_parameters, params, headers, json_payload, parser, payload_list, payload_type, post_data, files, require_auth, return_cursors, upload_api, use_cache, **kwargs)\u001b[0m\n\u001b[0;32m    257\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mUnauthorized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m403\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mForbidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m404\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mNotFound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mForbidden\u001b[0m: 403 Forbidden\n195 - Missing or invalid url parameter."
     ]
    }
   ],
   "source": [
    "key1=[\" #\"]\n",
    "key1_tweet=get_tweet(key1)\n",
    "key1_tweet.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "44372bc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid!!\n",
      "Enter keyword without space or special character!!\n"
     ]
    }
   ],
   "source": [
    "key1=[\"policy\",\" $\"]\n",
    "a=1\n",
    "for k in key1:\n",
    "    if(re.search(p, k)):\n",
    "        a=0\n",
    "    \n",
    "if a==0:\n",
    "    print(\"Invalid!!\\nEnter keyword without space or special character!!\")\n",
    "else:\n",
    "    key1_tweet=get_tweet(key1)\n",
    "    key1_tweet.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3dbc7e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected parameter: since\n",
      "Unexpected parameter: since\n"
     ]
    }
   ],
   "source": [
    "key2=[\"hhj\",\"ds\"]\n",
    "b=1\n",
    "for k in key2:\n",
    "    if(re.search(p, k)):\n",
    "        b=0\n",
    "    \n",
    "if b==0:\n",
    "    print(\"Invalid\")\n",
    "else:\n",
    "    key2_tweet=get_tweet(key2)\n",
    "    key2_tweet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9828a09",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed421fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46612f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f0ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce035db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    #LowerCasing all tweets\n",
    "    tweet=tweet.lower()\n",
    "    #removing URL\n",
    "    tweet=re.sub(r\"http\\S+|www\\S+|https\\S+\",\"\",tweet)\n",
    "    #removing @ from tweets\n",
    "    tweet=re.sub(r'\\@\\w+|\\#+',\" \",tweet)\n",
    "    #removing punctuation from tweets\n",
    "    tweet=tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    #removing rt from tweets\n",
    "    tweet=re.sub(r'^rt[\\s]+', '', tweet)\n",
    "    #Stop Word removal\n",
    "    tweet_tokens=word_tokenize(tweet)\n",
    "    filtered_words=[word for word in tweet_tokens if word not in stop_words]\n",
    "    #lemmatization\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    lemma_words=[lemmatizer.lemmatize(w,pos='a') for w in filtered_words]\n",
    "    return \" \".join(lemma_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5507f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "key1_tweet['clean_tweets'] = key1_tweet['Tweet'].apply(lambda Tweet: preprocess_tweet(Tweet))\n",
    "key1_tweet[\"clean_tweets\"].replace(\"\", np.NaN, inplace=True)\n",
    "key1_tweet.dropna(inplace=True)\n",
    "key1_tweet.to_csv('key1_cleantweets.csv')\n",
    "key1_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9960f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "key2_tweet['clean_tweets'] = key2_tweet['Tweet'].apply(lambda Tweet: preprocess_tweet(Tweet))\n",
    "key2_tweet[\"clean_tweets\"].replace(\"\", np.NaN, inplace=True)\n",
    "key2_tweet.dropna(inplace=True)\n",
    "key2_tweet.to_csv('key2_cleantweets.csv')\n",
    "key2_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2666306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(sentence):\n",
    "    text = Sentence(sentence)\n",
    "    classifier.predict(text)\n",
    "    label = str(text.labels[0]).split()[0]\n",
    "    if label==\"NEGATIVE\":\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5bebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_classify(df):\n",
    "    df['sentiment'] = df['clean_tweets'].apply(lambda clean_tweets: sentiment_analysis(clean_tweets))\n",
    "    pos=(df[\"sentiment\"]==\"Positive\").sum()\n",
    "    neg=(df[\"sentiment\"]==\"Negative\").sum()\n",
    "    return pos,neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a44cf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos1,neg1=sentiment_classify(key1_tweet)\n",
    "key1_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b871e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=len(key1_tweet)\n",
    "print(\"Positive sentiment percentage: \",pos1*100/n)\n",
    "print(\"Negative sentiment percentage: \",neg1*100/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b74709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos2,neg2=sentiment_classify(key2_tweet)\n",
    "key2_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5c9183",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=len(key2_tweet)\n",
    "print(\"Positive sentiment percentage: \",pos2*100/n)\n",
    "print(\"Negative sentiment percentage: \",neg2*100/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c22a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "winner=key1 if (pos1>pos2) else key2\n",
    "print(winner,\"has a more positive sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429e914",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
